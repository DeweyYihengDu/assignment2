---
title: "Assignment2"
date: "`r Sys.Date()`"
author: Yiheng Du
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

Student ID: u7457260
Github Sharing: [The address of Github](https://github.com/DeweyYihengDu/assignment2)

# Introduction

## library the packages which will be used.

```{r results="hide"}
library(tidyverse)
library(metafor)
library(orchaRd)
library(here)
library(pacman)
library(flextable)
```

## Read the data
```{r}
data1 <- read_csv(here('data', 'OA_activitydat_20190302_BIOL3207.csv'))
```

# Do the meta-analysis 

## Analysis the data form Clark get the data of Mean, SD and M.

### Read the data and get the name of every fish.

```{r}
fish_name <- data1 %>% select(species) %>% unique()
```

We can get the fish name list: `{r} fish_name`

### Calculate the data about the Mean, SD and N.

```{r}
tidydata <- function(i){
  control <- data1 %>% 
    filter(species == i) %>%
    filter(treatment=="control") %>% 
    select(activity) %>% 
    drop_na()
  oa <- data1 %>% 
    filter(species == i) %>%
    filter(treatment=="CO2") %>% 
    select(activity) %>% 
    drop_na()
  Species = i
  ctrl.sd = sd(control$activity)
  ctrl.mean=mean(control$activity)
  ctrl.n=nrow(control)
  oa.sd = sd(oa$activity)
  oa.mean=mean(oa$activity)
  oa.n=nrow(oa)
  data.frame(Species,ctrl.mean,ctrl.sd,ctrl.n, oa.mean, oa.sd, oa.n)
}

a=rbind(tidydata("ambon"),tidydata("lemon"),tidydata("chromis"),tidydata("acantho"),tidydata("humbug"),tidydata("whitedams"))
```



## Merge the summary statistics

```{r}
clark_paper_data <- read_csv(here("data","clark_paper_data.csv"))
clark_paper_data
b = cbind(clark_paper_data,a)
```



## Merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset 

```{r}
ocean_meta_data <- read_csv(here("data","ocean_meta_data.csv"))
meta_data = rbind(b, ocean_meta_data)
```

Then, tidy the data and reneme some data.

```{r}
colnames(meta_data)[3] <- 'Year_online'
colnames(meta_data)[4] <- 'Year_print'
colnames(meta_data)[11] <- 'Climate'
colnames(meta_data)[15] <- 'Life_stage'
colnames(meta_data)[10] <- "Effect_type"
meta_data <- meta_data %>% mutate(residual = 1:n())
```

```{r}
head(meta_data)
```


# Start the meta analys

## Get the log response ration(LnRR)

```{r}
LnRR_data <- escalc(measure = "ROM", 
             n1i=ctrl.n, n2i = oa.n, m1i = ctrl.mean,m2i = oa.mean,sd1i = ctrl.sd,sd2i = oa.sd,
             data=meta_data, var.names = c("LnRR", "v"))
```



## Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR.


```{r}
ss <- rma.mv(LnRR~1, V=v,data=LnRR_data,method = "REML", 
             random = list(~1|Study, ~1|residual, ~1|Year_online, ~1|Climate,~1|Life_stage, ~1|Species,~1|Effect_type),
             dfs = "contain", test = "t")
summary(ss)
```




## The result 

### 95% Confidence Intervals in this model

```{r}
a=predict(ss, transf = "transf=exp")
a
```


The 95% prediction intervals are wide. LnRR are expected to range from `r a$ci.lb` to `r a$ci.ub` 95% of the time with repeated experiments, suggesting a lot of inconsistency between studies.

* The 95% confidence interval ranged from `r a$ci.lb` to `r a$ci.ub`.
* In 95% of the probability cases, the true value is expected to fall from `r a$ci.lb` to `r a$ci.ub`.
* In other words, if we were to repeat the experiment many times, 95% of the confidence intervals constructed would contain the true meta-analytic mean.


### Measures of heterogeneity in effect size estimates across studies 


```{r}
i2_vals <- orchaRd::i2_ml(ss)

i2 <- tibble(type = firstup(gsub("I2_", "", names(i2_vals))), I2 = i2_vals)

flextable(i2) %>%
    align(part = "header", align = "center") %>%
    compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
    compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")),
        as_b("(%)")))

```


* Overall, we have highly heterogeneous effect size data because the total $I^2$ is `r i2[1,2]`% . 
* From the multilevel meta-analysis model, we found that `r i2[2,2]`% of the total variation in effect size estimates was the result of between-study variation.
* From the multilevel meta-analysis model, we found that `r i2[4,2]`% of the total variation in effect size estimates was the result of between-Year_online variation.
* From the multilevel meta-analysis model, we found that `r i2[5,2]`% of the total variation in effect size estimates was the result of between-Climate variation.
* From the multilevel meta-analysis model, we found that `r i2[6,2]`% of the total variation in effect size estimates was the result of between-Life_stage variation.
* From the multilevel meta-analysis model, we found that `r i2[7,2]`% of the total variation in effect size estimates was the result of between-Effect_type variation.


```{r}
predict(ss)
```

The pre




### The forest plot about the meta analysis


```{r, fig.cap="Orchard plot of the main model"}
M1 <- rma.mv(LnRR~1, V=v,data=LnRR_data,method = "REML", 
             random = list(~1|Study, ~1|residual, ~1|Species, ~1|Year_online),
             dfs = "contain", test = "t")
orchaRd::orchard_plot(M1, group = "Study", mod = "1", data = LnRR_data, xlab = "ghghation Coefficient",angle = 45) + labs(title = "Fig1 Orchard plot of the main model")
```

From Fig1, we can find that k = the number of effect sizes and the number of studies are in brackets. k=801(92) means the number of studies is 801 and the number of effect sizes is 92. 
The mean value is the circle in the plot, nearly in the end of the plot. The 95% confidence interval is shown as a short bolded horizontal line in the graph.



```{r, fig.cap="Orchard plot about the Climate"}
M2Climate<- rma.mv(LnRR~Climate-1, V=v,data=LnRR_data,method = "REML", 
             random = list(~1|Study, ~1|residual,  ~1|Year_online),
             dfs = "contain", test = "t")
orchaRd::orchard_plot(M2Climate, group = "Study", mod = "Climate", data = LnRR_data, xlab = "ghghation Coefficient",angle = 45)+labs(title="The forest plot of the model of climate") + labs(title = "Fig2 Orchard plot about the Climate")
```

Form Fig2, it shows that the climate influence during the model of meta-regression. And the 

```{r, fig.cap="Orchard plot of the meta-regression model about Life Stage"}
M3Life_stage<- rma.mv(LnRR~Life_stage-1, V=v,data=LnRR_data,method = "REML", 
             random = list(~1|Study, ~1|residual,  ~1|Year_online),
             dfs = "contain", test = "t")
orchaRd::orchard_plot(M3Life_stage, group = "Study", mod = "Life_stage", data = LnRR_data, xlab = "ghghation Coefficient",angle = 45)+labs(title="Fig3 Orchard plot of the meta-regression model about Life Stage")
```

From the Fig3, the mean is a hollow circle in the middle and the 95% confidence interval is a short horizontal line thickened in the middle. `k` is the number of effect sizes and the number of studies are in brackets.

```{r, fig.cap="Orchard plot of the meta-regression model about effect type"}
M4Effect_type<- rma.mv(LnRR~Effect_type-1, V=v,data=LnRR_data,method = "REML", 
             random = list(~1|Study, ~1|residual,  ~1|Year_online),
             dfs = "contain", test = "t")
orchaRd::orchard_plot(M4Effect_type, group = "Study", mod = "Effect_type", data = LnRR_data, xlab = "ghghation Coefficient",angle = 45)+labs(title="The forest plot of the model of Effect type")
```

From the Fig4, the mean is a hollow circle in the middle and the 95% confidence interval is a short horizontal line thickened in the middle. `k` is the number of effect sizes and the number of studies are in brackets.


### Total result

From Fig1, we can find that most of the studies are going to have negative effects. 

From Fig2, different climates can produce large deviations in the results, and the side effects are obvious in deep environments.

From Fig3, the different life stages will have a certain bias on the results, with the juvenile stage producing more pronounced side effects. This is also consistent with the fact that larvae are more sensitive to environmental changes.

From Fig4, the effect type maybe can not influence the results. That is, there was no significant deviation from the impact type during the publication process. The reason for this may be that this type of research is still in its infancy and no traditional ideas about this type of research have arisen.

## Visual inspections of possible publication bias – funnel plots

```{r, fig.cap="Funnel plot showing the precision of effects against their correlation"}
funnel(x=LnRR_data$LnRR, vi = LnRR_data$v, yaxis = "seinv", ylim = c(0.00001,60),xlim = c(-2,2),
    digits = 3, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"),
    las = 1, xlab = "Correlation Coefficient (r)", atransf = tanh, legend = TRUE) 
```

From the Fig5, it can be seen that the graph is typical of a funnel trait. 

Most of the effects are present in the positive correlation space. 

But at the same time there are also many studies outside the positive correlation space.

Moreover, we can find that articles can be published in cases where the relevance is particularly high, which means that the apparently authored results can help increase the success rate of publication.

```{r, fig.cap="The plot of Sampling Variance of LnRR against LnRR"}
aaa <- LnRR_data %>% filter(v < 20)
ggplot(aaa, aes(y = LnRR, x = v)) + geom_point() + geom_smooth(method = "lm") +
    labs(y = "LnRR", x = "Sampling Variance of LnRR",title="Fig 6 The plot of Sampling Variance of LnRR against LnRR")+
    theme_classic()
```

Form Fig6, it is not difficult to see the line is sloped upward. And there were more publications with stronger side effects, so we inferred that there was a publication bias. It may be that the very strong side effects make more sense to the authors or the reviewers.

```{r}
mly <- rma.mv(LnRR~v, V=v,data=LnRR_data,method = "REML", 
             random = list(~1|Study, ~1|residual, ~1|Species),
             dfs = "contain", test = "t")
mly
```

From the result of meta_regession mode, it shows that if the v as a moderator, the accuracy of the model can be imporoved because the estimate is -0.0611 which is near 0.


```{r}
r2 <- orchaRd::r2_ml(mly)
r2
```

And we can find that the R2_conditional is lager than the R2_marginal. We can see that sampling variance explains 2.79% of effect size variance. This is the marginal $R^2$, which tell us how much variation the ‘fixed effects’ or moderators explain in the model. Conditional $R^2$ tells us that the full model, that accounts for the both the fixed and random effects, explains 95.28% of variance in effect size.


## Fitting a Multilevel Meta-Regression model to Test and Correct for Publication bias


```{r, fig.cap="Plot of LnRR as a function of publication year"}
ggplot(LnRR_data, aes(y = LnRR_data$LnRR, x =LnRR_data$Year_online, size = 1/sqrt(v))) + geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", col = "red", show.legend = FALSE) + 
    labs(x = "Publication Year",y = "LnRR", size = "Precision (1/SE)") +
  geom_hline(yintercept = 0)+
    theme_classic()+
  labs(title = "Fig7 Plot of LnRR as a function of publication year")

```

1. There does appear to be a little positive relationship with year.
2. From the figure, we can learn that as time increases, we find that there does not seem to be much change in the accuracy of the sampling.
3. Also of note are that the time near 2012 studies have much higher sampling variance(i.e., lower precision), maybe there are something happened this time.
4. The reason for the high accuracy in the earliest studies may be due to the fact that the researchers who first discovered the phenomenon conducted experiments with huge sample sizes in order to prove the existence of the phenomenon, thus making it highly accurate.
5. However, the large variance of the early studies may be due to the lack of standardized operational procedures and analysis methods developed in the early years, resulting in less accurate results in the previous years.
6. As the year increases, the fitted straight line gradually crosses the 0-axis line, which means that the variance becomes progressively smaller and the effect of the effect becomes progressively more pronounced.
7. In recent years, the curve has flattened out, implying that the influence of year on this study has gradually diminished and that the methodology of this type of study is maturing. 


## Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias

```{r}
arnold_data_endo <- LnRR_data %>%
    mutate(Year_c = Year_online - mean(Year_online))
metareg_time_c <- rma.mv(LnRR ~ Year_c-1, V = v, random = list(~1 | Study), test = "t", dfs = "contain", data = arnold_data_endo)
summary(metareg_time_c) 
```

Subtracting the year from the average of the years to achieve the effect of pooling the years and as a moderator is found to improve the accuracy of meta-model.

```{r}
metareg_time <- rma.mv(LnRR ~ Year_online-1, V = v, random = list(~1 | Study), test = "t", dfs = "contain", data = arnold_data_endo)
r2_time <- orchaRd::r2_ml(metareg_time)  
r2_time
```

We can see that sampling variance explains 31.32% of effect size variance. This is the marginal $R^2$, which tells us how much variation the ‘fixed effects’ or moderators explain in the model. Conditional $R^2$ tells us that the full model, which accounts for both the fixed and random effects, explains 72.14% of the variance in effect size.

It is useful to use year as a moderator. 



## Formal meta-regression model that includes inverse sampling variance to test for file-drawer biases

Use the v and LnRR as mators to formal meta-regression model. 

I use the $1/(LnRR * vi)$ to link them.

```{r}
lastone <- rma.mv(LnRR ~ 1/(v*LnRR), V = v, random = list(~1 | Study), test = "t", dfs = "contain", data = arnold_data_endo)
summary(lastone)
```

From the result of meta-regression model, we can get the estimate is very little so that it is helpful for the modelling.

# Discussion

## The result of meta-analysis

There appears to be a temporal publication bias in the data. In the beginning it may have been due to the low importance given to the results of studies with small samples, which led to a large sample size in some of the first published articles. As time progresses, the experimental samples and effects gradually tend toward the mean. The large variance in the first few years may be due to the immaturity of the experimental methods and techniques, resulting in large experimental results.

Climate and life stage are also the publication biases of such studies. Among the climates, deep climate has the most bias. The reason for the higher bias in deep may be that there are more species studied in this category and there is a better research base. There are two possible reasons for more deviations in juveniles: one is that juveniles are more sensitive to external influences, and the other is that juveniles are easier to study and more obvious results can easily appear.

The effect type has a small effect on publication, and the number of publications is high for both positive and negative effects and no effect. The reason for this may be that this type of research is still in the development stage and no traditional view has been formed. All types of studies are in an equal state and the acceptance of their findings is high.


## Compare with a meta-analysis by others.

In the study of fish behavior, we can find many publication biases at present, these include years, climate, and life stage. Compared with the result of [Cleement et. al.(2022)](https://pubmed.ncbi.nlm.nih.gov/35113875/), I add the years to this deviation during the model of meta-regression analysis. From the Years of the possibility of publication bias, the bias is larger in the earlier years but decreases as the year's increase, which represents that such studies are becoming more standardized and the results do not produce more variation.

The results are consistent with his study in that there is a large deviation in the direction of climate and life stage. The publication bias may be influenced by the simplicity of the studies on the non-larval stage or the greater experience in pre-coldwater fish studies. 

There are two possibilities for the lack of publication bias in effect style: one is that the research is not yet mature and people have not yet formed traditional ideas about this type of research, and the other possibility is that the research is already very mature and there is more inclusiveness about the results of this type of research, so there is no publication bias about effect style.

And combined with the above discussion, we can learn that the current research at this stage is still in the development stage, without the formation of a more solid traditional concept. There is still specificity in the selection of some, for example, climate and lifestyle, so there are large deviations in these areas. If this situation persists, it may result in some duplicate or similar experiments, which may create waste. And under the influence of some, for example, ocean acidification, no significant deviations have been found for such effects.

In general, the majority of the results show that $CO_2$ rise reduces the activity of fish, but there are more differences between different climates or lifestyles. Therefore, in future studies, we should focus on different species, different living environments, and different life states to achieve additional clarification in this field.


# Reference

[1]Clements JC, Sundin J, Clark TD, Jutfelt F. Meta-analysis reveals an extreme "decline effect" in the impacts of ocean acidification on fish behavior. PLoS Biol. 2022 Feb 3;20(2):e3001511. doi: 10.1371/journal.pbio.3001511. PMID: 35113875; PMCID: PMC8812914.



